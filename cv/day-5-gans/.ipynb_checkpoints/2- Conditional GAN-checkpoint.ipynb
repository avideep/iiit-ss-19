{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Conditional Generative Adversarial Networks\n",
    "(Link to paper: https://arxiv.org/abs/1411.1784)In Conditional GANs, we feed a 'condition' to both the generator and discriminator during training. This will allow us to generate images fulfilling a condition, by feeding the condition to the generator, along with the random noise.We will use MNIST data for this experiment. We will try to condition the network using the class labels of MNIST data. Thus we should be able to generate images from a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda')\n",
    "%matplotlib inline\n",
    "\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    vec = torch.randn(*size) * xavier_stddev\n",
    "    vec = vec.to(device)\n",
    "    vec.requires_grad = True\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will feed the condition as a 'one hot vector encoding' of the class label. The functions below load the MNIST dataset and convert the label into a one hot encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "dataroot = 'data'\n",
    "# load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0,), (1,))])\n",
    "trainset = torchvision.datasets.MNIST(root=dataroot, train=True,\n",
    "                                        download=False,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')\n",
    "\n",
    "def onehotencoder(x):\n",
    "    y = np.zeros((x.numpy().shape[0],10))\n",
    "    for i in range(x.numpy().shape[0]):\n",
    "        y[i,x[i]] = 1\n",
    "    return y\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "def mnist_next(dataiter):\n",
    "\n",
    "    try:\n",
    "        images, labels = dataiter.next()\n",
    "        labels = onehotencoder(labels)\n",
    "        images = images.view(images.numpy().shape[0],28*28)\n",
    "    except:\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        labels = onehotencoder(labels)\n",
    "        images = images.view(images.numpy().shape[0],28*28)\n",
    "    return images.numpy(), labels\n",
    "\n",
    "def initialize_loader(trainset):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    dataiter = iter(trainloader)\n",
    "    return dataiter\n",
    "\n",
    "images, labels = mnist_next(dataiter)\n",
    "# print some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "img = torchvision.utils.make_grid(images)\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to pass the condition to the generator and discriminator, we have to represent them in a 'joint vector representation'. For our joint vector representation, we will concatenate the one-hot encoded vector with the noise vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "\n",
    "bzvar = torch.zeros(h_dim)\n",
    "bhvar = torch.zeros(X_dim)\n",
    "\n",
    "bzvar = bzvar.to(device)\n",
    "bhvar = bhvar.to(device)\n",
    "\n",
    "bzh = Variable(bzvar, requires_grad=True)\n",
    "bhx = Variable(bhvar, requires_grad=True)\n",
    "\n",
    "def G(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = nn.relu(torch.mm(inputs, Wzh) + bzh.repeat(inputs.size(0), 1))\n",
    "    X = nn.sigmoid(torch.mm(h, Whx) + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Similarly, for the discriminator, we will concatenate the one hot encoded vectors with the flattened image to get the joint vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bxvar = torch.zeros(h_dim)\n",
    "bhvar = torch.zeros(1)\n",
    "bxvar = bxvar.to(device)\n",
    "bhvar = bhvar.to(device)\n",
    "    \n",
    "bxh = Variable(bxvar, requires_grad=True)\n",
    "bhy = Variable(bhvar, requires_grad=True)\n",
    "\n",
    "def D(X, c):\n",
    "    inputs = torch.cat([X, c], 1)\n",
    "    h = nn.relu(torch.mm(inputs, Wxh) + bxh.repeat(inputs.size(0), 1))\n",
    "    y = nn.sigmoid(torch.mm(h, Why) + bhy.repeat(h.size(0), 1))\n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us extract the parameters for training and create the ADAM solvers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = torch.ones(mb_size)\n",
    "zeros_label = torch.zeros(mb_size)\n",
    "\n",
    "ones_label = ones_label.to(device)\n",
    "zeros_label = zeros_label.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, we do the actual training. It is similar to the vanilla GAN network, except that we concatenate the one-hot encoded labels to the inputs of both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = initialize_loader(trainset)\n",
    "\n",
    "for it in range(10000):\n",
    "    # Sample data\n",
    "    z =torch.randn(mb_size, Z_dim)\n",
    "    X, c = mnist_next(dataiter)\n",
    "    if X.shape[0]!=mb_size:\n",
    "        dataiter = initialize_loader(trainset)\n",
    "        X,c = mnist_next(dataiter)\n",
    "    X = torch.from_numpy(X)\n",
    "    c = torch.from_numpy(c.astype('float32'))\n",
    "    \n",
    "    z = z.to(device)\n",
    "    X = X.to(device)\n",
    "    c = c.to(device)\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z, c)\n",
    "    D_real = D(X, c)\n",
    "    D_fake = D(G_sample, c)\n",
    "\n",
    "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = torch.randn(mb_size, Z_dim)\n",
    "    z = z.to(device)\n",
    "\n",
    "    G_sample = G(z, c)\n",
    "    D_fake = D(G_sample, c)\n",
    "\n",
    "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.cpu().numpy(), G_loss.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the images produced by the C-GAN. By changing the condition vector, we can generate images from different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(mb_size, Z_dim)\n",
    "discr_codes = torch.zeros(64,10)\n",
    "z = z.to(device)\n",
    "discr_codes = discr_codes.to(device)\n",
    "\n",
    "\n",
    "discr_codes[:,4] = 1 \n",
    "samples = G(z,discr_codes)\n",
    "samples = samples.cpu()\n",
    "img = samples.data\n",
    "img = img.view([64,1,28,28])\n",
    "img = torchvision.utils.make_grid(img)\n",
    "img = img.permute(1,2,0)\n",
    "plt.imshow(img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Exercises:\n",
    "<ol>\n",
    "<li> What do the noise vectors mean? What happens when we gradually change the noise vectors? </li>\n",
    "<li> How does the plot of error compare to vanilla GAN?</lo>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
