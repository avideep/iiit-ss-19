{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Semantic Segmentation: Let's classify each pixel</center></h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.) What is segmentation ?\n",
    "\n",
    "The aim of **segmentation** is to partition an image into a collection of set of pixels. \n",
    "\n",
    "What could be such a collection of pixels:\n",
    "\n",
    "* Meaningful regions (coherent objects)\n",
    "\n",
    "* Linear Structures (line, curve,...)\n",
    "\n",
    "* Shapes (circles, ellipses,...)\n",
    "\n",
    "<img src=\"img/seg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not all. What if we want to explore more and want know pixels which occupy which category amongst the defined ones.\n",
    "\n",
    "**Semantic Segmentation:** Semantic Segmentation of an image is the task to assign each pixel in the input image a semantic class in order to get a pixel-wise dense classification. \n",
    "<img src=\"img/ss_tot.png\">\n",
    "\n",
    "Traditonal methods do feature representation and go up in a hierarchial way to do the segmentation.With the popularity of deep learning in recent years, many semantic segmentation problems are being tackled using deep architectures, most often Convolutional Neural Nets, which surpass other approaches by a large margin in terms of accuracy and efficiency. Breakthrough came in this field of semantic segmentation when **Fully Convolutional Networks** were first introduced in 2014 **Long et. al.** to perform end-to-end segmentation of natural images.\n",
    "<img src=\"img/fcn.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Applications of Semantic Segmentation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic segmentation is generally used for:\n",
    "\n",
    "-  Autonomous driving (Scene understanding)\n",
    "-  Medical image segmentation\n",
    "-  Robot Vision and Understanding\n",
    "\n",
    "The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include self-driving vehicles, human-computer interaction, virtual reality etc.\n",
    "\n",
    "We are going to look at some of the dataset which is widely used for the task of scene understanding and hence apply our semantic segmnetation algorithm on the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Cityscapes Dataset** focuses on semantic understanding of urban street scenes. This dataset has been collected over 50 cities of Germany in different settings. It contains 2975 training and 500 validation images with publicly available annotations, as well as 1525 test images. The dataset contains fine and coarse annotations that have been manually annotated. \n",
    "\n",
    "The authors came up with 30 visual classes for annotation, which are grouped  into  eight  categories: flat, construction, nature, vehicle, sky, object, human, and void. (Please see the image below from their paper!!)\n",
    "\n",
    "<img src=\"img/class.png\">\n",
    "\n",
    "Image | Color Mask\n",
    "- | - \n",
    "![alt](img/aachen.png) | ![alt](img/aachen_color.png)\n",
    "![alt](img/zurich.png) | ![alt](img/zurich_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics used in evaluation of Semantic Segmentation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different metrics used for evaluating semantic segmenatation algorithm are given below in the picture: \n",
    "<center> Pixel Accuracy </center> | <center> Mean Accuracy </center>\n",
    "- | -\n",
    "![alt](img/pa_ss.png) | ![alt](img/ma_ss.png)\n",
    "\n",
    "<center> Mean IOU </center> | <center> Frequency Weighted IOU </center>\n",
    "- | -\n",
    "![alt](img/miou_ss.png) | ![alt](img/fiou_ss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center> Model Architectures for Semantic Segmentation</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, there are two types of architectures that are prominent in semantic segmentation.\n",
    "\n",
    "* Encoder Decoder Based Architecture\n",
    "\n",
    "**Encoder**: A pre-trained classification network like VGG/ResNet followed by a decoder network. \n",
    "\n",
    "**Decoder**: The task of the decoder is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification. This is where most architectures differ. Some architectures directly upsample the feature map and some use complex architecture to get more refined outputs. \n",
    "\n",
    "\n",
    "* Spatial Pyramid Pooling Based Architecture\n",
    "\n",
    "  **Extract features from CNN based architecture**\n",
    "\n",
    "  **Sub region pooling** \n",
    "\n",
    "  **1X1 Convolution for feature concatenation/ dimension reduction**\n",
    "\n",
    "  **Interpolation for upsampling**\n",
    "\n",
    "Take a look at the architectures !!\n",
    "\n",
    "<center> Spatial Pyramid Pooling Architecture</center> | <center> Encoder Decoder Architecture </center>\n",
    "- | -\n",
    "![alt](img/spp.png) | ![alt](img/enc.png)\n",
    "\n",
    "\n",
    "<img src=\"img/spp_2.png\">\n",
    "<img src=\"img/encode_ss.jpeg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries needed for processing\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library to do image related operations\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the important functionalities of Pytorch such as the dataloader, Variable, transform's \n",
    "#and optimizer related functions.\n",
    "\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import  Resize\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset class for VOC12 and cityscapes\n",
    "from dataset import cityscapes\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "#Importing the Relabel, ToLabel and Colorize class from transform.py file\n",
    "from transform import Relabel, ToLabel, Colorize\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from iouEval import iouEval, getColorEntry #importing iouEval class from the iouEval.py file\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few global parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 #RGB Images\n",
    "NUM_CLASSES = 8 #cityscapes=8 # Since we going for categories, therefore classes are 8, please check the figure in the above cells\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "IMAGE_HEIGHT = 128\n",
    "DATA_ROOT = '/ssd_scratch/cvit/ashutosh.mishra/cityscapes/'\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 1\n",
    "ENCODER_ONLY = True\n",
    "#device = torch.device(\"cuda\" )\n",
    "device = 'cpu'\n",
    "color_transform = Colorize(NUM_CLASSES)\n",
    "image_transform = ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOUTRAIN = False\n",
    "IOUVAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentations - different function implemented to perform random augments on both image and target\n",
    "class MyCoTransform(object):\n",
    "    def __init__(self, enc, augment=True, height=512):\n",
    "        self.enc=enc\n",
    "        self.augment = augment\n",
    "        self.height = height\n",
    "        pass\n",
    "    def __call__(self, input, target):\n",
    "        # Resizing data to required size\n",
    "        input =  Resize(self.height, Image.BILINEAR)(input)\n",
    "        target = Resize(self.height, Image.NEAREST)(target)\n",
    "\n",
    "        if(self.augment):\n",
    "            # Random horizontal flip\n",
    "            hflip = random.random()\n",
    "            if (hflip < 0.5):\n",
    "                input = input.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            #Random translation 0-2 pixels (fill rest with padding)\n",
    "            transX = random.randint(0, 2) \n",
    "            transY = random.randint(0, 2)\n",
    "\n",
    "            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\n",
    "            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=7) #pad label filling with 7\n",
    "            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\n",
    "            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))   \n",
    "\n",
    "        input = ToTensor()(input)\n",
    "        \n",
    "        target = ToLabel()(target)\n",
    "        \n",
    "        target = Relabel(0,255)(target) #void\n",
    "        target = Relabel(1,0)(target) #flat\n",
    "        target = Relabel(2,1)(target) #construction\n",
    "        target = Relabel(3,2)(target) #object\n",
    "        target = Relabel(4,3)(target) #nature\n",
    "        target = Relabel(5,4)(target) #sky\n",
    "        target = Relabel(6,5)(target) #human \n",
    "        target = Relabel(7,6)(target) #vehicle\n",
    "        target = Relabel(255,7)(target)\n",
    "        return input, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "We'll follow pytorch recommended semantics, and use a dataloader to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "co_transform = MyCoTransform(ENCODER_ONLY, augment=True, height=IMAGE_HEIGHT)#1024)\n",
    "co_transform_val = MyCoTransform(ENCO\n",
    "                                 \\\\DER_ONLY, augment=False, height=IMAGE_HEIGHT)#1024)\n",
    "\n",
    "#train data\n",
    "dataset_train = cityscapes(DATA_ROOT, co_transform, 'train')\n",
    "print(len(dataset_train))\n",
    "#test data\n",
    "dataset_val = cityscapes(DATA_ROOT, co_transform_val, 'val')\n",
    "\n",
    "# NOTE: PLEASE DON'T CHANGE batch_size and num_workers here. We have limited resources.\n",
    "loader_train = DataLoader(dataset_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(len(loader_train))\n",
    "loader_val = DataLoader(dataset_val, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy  Loss ##\n",
    "Negative Log Loss   |Plot of -log(x) vs x\n",
    "- | - \n",
    "![alt](img/nll.png) | ![alt](img/nll-log.png)\n",
    "\n",
    "The negative log-likelihood becomes unhappy at smaller values, where it can reach infinite unhappiness (that’s too sad), and becomes less unhappy at larger values. Because we are summing the loss function to all the correct classes, what’s actually happening is that whenever the network assigns high confidence at the correct class, the unhappiness is low, but when the network assigns low confidence at the correct class, the unhappiness is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the data? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "print(len(loader_train))\n",
    "print(len(loader_val))\n",
    "dataiter = iter(loader_train)\n",
    "#print(dataiter.next())\n",
    "(images, labels) = dataiter.next()\n",
    "#for step, (images, labels) in enumerate(loader_train):\n",
    "plt.figure()\n",
    "plt.imshow(ToPILImage()(images[0].cpu()))\n",
    "plt.figure()\n",
    "plt.imshow(ToPILImage()(Colorize()(labels[0].cpu())))  \n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we would be using [ERFNET](http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17iv.pdf)\n",
    "model, which is an encoder-decoder architecture. Please see the image below !!\n",
    "<img src=\"img/erfnet.png\">\n",
    "<img src=\"img/erfnet-model.png\">\n",
    "\n",
    "\n",
    "We show two ways to train the **ERFNET architecture** on a standard dataset:\n",
    "\n",
    "-  Training the encoder and decoder from the scratch.\n",
    "-  Just training the encoder and then bilinearly upsampling the encoder's output to match input image size .\n",
    "\n",
    "ERFNET reformulates the **Resnet architecture** into novel 1-d bottleneck and non bottleneck 1-d blocks (See more in the paper). Because of the ResNet architecture, the information of semantic boundary is passed through the residual mappings and hence the need for explicit skip connections is not required.   \n",
    "\n",
    "Downsampling (reducing the spatial resolution) has the drawback of reducing the accuracy (coarser outputs), but it\n",
    "also  has  two  benefits:  it  lets  the  deeper  layers  gather  more context  (to  improve  classification)  and  it  helps  to  reduce computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = 'erfnet'\n",
    "\n",
    "if model_selection == 'drnet':\n",
    "    model_file = importlib.import_module('drnet')\n",
    "    model = model_file.Net(NUM_CLASSES, 'resnet_18')\n",
    "elif model_selection == 'erfnet':\n",
    "    model_file = importlib.import_module('erfnet')\n",
    "    model = model_file.Net(NUM_CLASSES)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use adam optimizer. It can be replaced with SGD and other optimizers\n",
    "optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4) \n",
    "start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CUDA:\n",
    "        model = torch.nn.DataParallel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "steps_loss = 50\n",
    "my_start_time = time.time()\n",
    "for epoch in range(start_epoch, NUM_EPOCHS+1):\n",
    "    print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\n",
    "\n",
    "    epoch_loss = []\n",
    "    time_train = []\n",
    "\n",
    "    doIouTrain = IOUTRAIN   \n",
    "    doIouVal =  IOUVAL      \n",
    "\n",
    "    if (doIouTrain):\n",
    "        iouEvalTrain = iouEval(NUM_CLASSES)\n",
    "\n",
    "    model.train()\n",
    "    for step, (images, labels) in enumerate(loader_train):\n",
    "\n",
    "        start_time = time.time()\n",
    "        inputs = images.to(device)\n",
    "        targets = labels.to(device)\n",
    "        #values, indices = targets[0].max(0)\n",
    "        print('MAX', torch.max(targets))\n",
    "        #print(values,indices)\n",
    "        outputs = model(inputs, only_encode=ENCODER_ONLY)\n",
    "        #outputs = model(inputs)  \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        loss = criterion(outputs, targets[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        time_train.append(time.time() - start_time)\n",
    "\n",
    "        if (doIouTrain):\n",
    "            #start_time_iou = time.time()\n",
    "            iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
    "            #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)      \n",
    "\n",
    "        # print statistics\n",
    "        if steps_loss > 0 and step % steps_loss == 0:\n",
    "            average = sum(epoch_loss) / len(epoch_loss)\n",
    "            print('loss: {average:0.4} (epoch: {epoch}, step: {step})', \"// Avg time/img: %.4f s\" % (sum(time_train) / len(time_train) / BATCH_SIZE))\n",
    "\n",
    "\n",
    "    average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "    iouTrain = 0\n",
    "    if (doIouTrain):\n",
    "        iouTrain, iou_classes = iouEvalTrain.getIoU()\n",
    "        iouStr = getColorEntry(iouTrain)+'{:0.2f}'.format(iouTrain*100) + '\\033[0m'\n",
    "        print (\"EPOCH IoU on TRAIN set: \", iouStr, \"%\")  \n",
    "my_end_time = time.time()\n",
    "print(my_end_time - my_start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate on 500 val images after each epoch of training\n",
    "print(\"----- VALIDATING - EPOCH\", epoch, \"-----\")\n",
    "model.eval()\n",
    "epoch_loss_val = []\n",
    "time_val = []\n",
    "\n",
    "if (doIouVal):\n",
    "    iouEvalVal = iouEval(NUM_CLASSES)\n",
    "\n",
    "for step, (images, labels) in enumerate(loader_val):\n",
    "    start_time = time.time()\n",
    "\n",
    "    inputs = images.to(device)    \n",
    "    targets = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #outputs = model(inputs, only_encode=ENCODER_ONLY) \n",
    "        outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets[:, 0])\n",
    "    epoch_loss_val.append(loss.item())\n",
    "    time_val.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "    #Add batch to calculate TP, FP and FN for iou estimation\n",
    "    if (doIouVal):\n",
    "        #start_time_iou = time.time()\n",
    "        iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
    "        #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)\n",
    "        \n",
    "    if steps_loss > 0 and step % steps_loss == 0:\n",
    "        average = sum(epoch_loss_val) / len(epoch_loss_val)\n",
    "        print('VAL loss: {average:0.4} (epoch: {epoch}, step: {step})', \n",
    "                \"// Avg time/img: %.4f s\" % (sum(time_val) / len(time_val) / BATCH_SIZE))\n",
    "\n",
    "\n",
    "average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\n",
    "\n",
    "iouVal = 0\n",
    "if (doIouVal):\n",
    "\n",
    "    iouVal, iou_classes = iouEvalVal.getIoU()\n",
    "    print(iou_classes)\n",
    "    iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n",
    "    print (\"EPOCH IoU on VAL set: \", iouStr, \"%\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Visualizing the Output###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Analysis\n",
    "dataiter = iter(loader_val)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "if USE_CUDA:\n",
    "    images = images.to(device)\n",
    "\n",
    "inputs = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, only_encode=ENCODER_ONLY)\n",
    "\n",
    "label = outputs[0].max(0)[1].byte().cpu().data\n",
    "\n",
    "label_color = Colorize()(label.unsqueeze(0))\n",
    "\n",
    "label_save = ToPILImage()(label_color)\n",
    "plt.figure()\n",
    "plt.imshow(ToPILImage()(images[0].cpu()))\n",
    "plt.figure()\n",
    "plt.imshow(label_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
